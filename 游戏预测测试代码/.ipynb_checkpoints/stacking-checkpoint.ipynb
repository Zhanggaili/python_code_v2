{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对pay_price大于0的样本进行2分类训练以及stacking分类预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_train_ = pd.read_csv(\"pay_pricedayu0.csv\")\n",
    "# data_train = pd.read_csv(\"pay_pricedengyu0.csv\")\n",
    "dt1=pd.to_datetime(data_train_[\"register_time\"])\n",
    "data_train_[\"register_time\"] = dt1.dt.dayofyear\n",
    "if \"prediction_pay_price\" in data_train_.columns:\n",
    "    data_train_ = data_train_.drop(\"prediction_pay_price\",axis=1) \n",
    "data_train_1 = data_train_.copy()\n",
    "data_train_ = data_train_.iloc[:,1:]\n",
    "data_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split   # cross_validation\n",
    "from sklearn.linear_model import RidgeClassifierCV,RidgeClassifier,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier,BaggingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB,GaussianNB\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier,NearestCentroid,KNeighborsClassifier\n",
    "from sklearn.semi_supervised import LabelPropagation,LabelSpreading\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis,LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class add_feature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, additional=1):\n",
    "        self.additional = additional\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X[\"shaman_rate\"] = X[\"shaman_reduce_value\"]/(X[\"shaman_add_value\"]+0.1)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "pipe = Pipeline([\n",
    "        ('add_feature', add_feature(additional=2))\n",
    "    ])\n",
    "\n",
    "data_train_ = pipe.fit_transform(data_train_)\n",
    "data_train_['lable_2'] = data_train_['lable']\n",
    "del data_train_['lable']\n",
    "data_train_.rename(columns={'lable_2':'lable'}, inplace = True)\n",
    "\n",
    "# data_train_.to_csv('tezhengceshi.csv',index=None)\n",
    "# data_train_['pvp_lanch*pay_count>mean'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train_\n",
    "x, y = data_train.iloc[:, 0:len(data_train.columns)-1], data_train.iloc[:,len(data_train.columns)-1:]\n",
    "x.shape,y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test=pd.read_csv('test_pay_pricedayu0.csv')\n",
    "fit_data = all_test.iloc[:,1:108]\n",
    "dt1=pd.to_datetime(fit_data[\"register_time\"])\n",
    "fit_data[\"register_time\"] = dt1.dt.dayofyear\n",
    "fit_data = pipe.fit_transform(fit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def stacking(x_train, y_train, x_test, cv):\n",
    "\n",
    "#     le = LabelEncoder()\n",
    "#     y_train = le.fit_transform(y_train)\n",
    "\n",
    "    clfs = [\n",
    "        xgb.XGBClassifier(max_depth = 5),\n",
    "        lgb.LGBMClassifier(max_depth = 5),\n",
    "        RandomForestClassifier(max_depth = 6),\n",
    "        GradientBoostingClassifier(max_depth = 5),\n",
    "    ]\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=cv)\n",
    "    skf_dataset = list(skf.split(x_train, y_train))\n",
    "\n",
    "    # y_count: the kinds of labels\n",
    "    y_count = len(set(y_train.lable))\n",
    "#     print y_count\n",
    "    \n",
    "    # blend_train is the probabilities that every clf predicts every label (i.e. y) for every sample\n",
    "    # it is used to train the clfs in the second layer\n",
    "    blend_train = np.zeros((x_train.shape[0], len(clfs) * y_count))\n",
    "    blend_train = pd.DataFrame(blend_train)\n",
    "\n",
    "    # blend_test is used as the input of the clfs in the second layer to predict the labels of x_test\n",
    "    blend_test = np.zeros((x_test.shape[0], len(clfs) * y_count))\n",
    "    blend_test = pd.DataFrame(blend_test)\n",
    "\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print j, clf\n",
    "\n",
    "        # blend_test_j: the probabilities that j-th clf predicts every label for x_test\n",
    "        blend_test_j = np.zeros((x_test.shape[0], len(skf_dataset) * y_count))\n",
    "        blend_test_j = pd.DataFrame(blend_test_j)\n",
    "        \n",
    "        for k, (train_idx, test_idx) in enumerate(skf_dataset):\n",
    "            x_train_k = x_train.iloc[train_idx,:]\n",
    "            y_train_k = y_train.iloc[train_idx,:]\n",
    "            x_train_holdout = x_train.iloc[test_idx,:]\n",
    "\n",
    "            clf.fit(x_train_k, y_train_k)\n",
    "            x_train_holdout_ = clf.predict_proba(x_train_holdout)\n",
    "            \n",
    "            #blend_train[test_idx, j*y_count: (j+1)*y_count] = clf.predict_proba(x_train_holdout)\n",
    "            blend_train.iloc[test_idx,j*y_count:(j+1)*y_count] = clf.predict_proba(x_train_holdout)\n",
    "            blend_test_j.iloc[:, k*y_count: (k+1)*y_count] = clf.predict_proba(x_test)\n",
    "\n",
    "        # because there are len(skf_dataset) blend_test_j for x_test, it needs to calculated the mean value\n",
    "        blend_test_j_mean = np.zeros((x_test.shape[0], y_count))\n",
    "        blend_test_j_mean = pd.DataFrame(blend_test_j_mean)\n",
    "\n",
    "        # indices: supposed y_count = 3, indices would be [0, 3, 6]\n",
    "        # it is used to find the corresponding probabilities of the same label, and calculate the mean\n",
    "        indices = np.arange(len(skf_dataset)) * y_count\n",
    "        for c in xrange(y_count):\n",
    "            blend_test_j_mean.iloc[:,c] = pd.DataFrame(blend_test_j.iloc[:, indices].mean(1))\n",
    "            indices += 1\n",
    "        blend_test[range(j*y_count,(j+1)*y_count)] = blend_test_j_mean\n",
    "#         print blend_test_j_mean.head()\n",
    "#         print blend_test.head()\n",
    "\n",
    "#     clf = LogisticRegression()\n",
    "#     clf.fit(blend_train, y_train)\n",
    "#     y_pred = clf.predict(blend_test)\n",
    "#     y_pred = le.inverse_transform(y_pred)\n",
    "\n",
    "    return blend_train,blend_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_train,stacking_test = stacking(x,y,fit_data,4)\n",
    "stacking_train['user_id'] = data_train_1['user_id']\n",
    "stacking_test['user_id'] = all_test['user_id']\n",
    "stacking_train['lable'] = data_train_1['lable']\n",
    "stacking_train.to_csv('stacking_train12.csv',index=None)\n",
    "stacking_test.to_csv('stacking_test12.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_train12 = stacking_train\n",
    "stacking_test12 = stacking_test\n",
    "stacking_train12_ = stacking_train12.iloc[:,:8]\n",
    "stacking_test12_ = stacking_test12.iloc[:,:8]\n",
    "# stacking_train12_.columns,stacking_test12_.columns\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stacking_train12_, stacking_train12.lable)\n",
    "y_pred = lr.predict(stacking_test12_)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "stacking_test12['lable'] = y_pred\n",
    "stacking_test12__ =stacking_test12.iloc[:,[8,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_test12__['lable'] = stacking_test12__.apply(lambda x: 2 if(x['lable'] == 0) else 1,axis=1)   \n",
    "stacking_test12__.to_csv('stacking_test12_lr.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
